## 第三课 GFS 案例学习

[Google File System（GFS）](https://pdos.csail.mit.edu/6.824/papers/gfs.pdf)

为什么读这篇论文?

1. map/reduce 文件系统应用
2. 存储失败的案例学习，为了简单和性能牺牲一些一致性，追加设计的动机。
3. 性能优秀：并行 I/O 性能
4. 系统好论文：从应用到网络的各种细节，所有 6.824 的课程主题（性能，容错，一致性）在本篇论文中都有论述。

什么是一致性？

1. 一个正确性的条件
2. 一致性对于数据复制和应用并发获取很重要，如果一个应用正在执行写操作，而后它再执行读操作应该读到什么？如果是另外一个应用读，又会怎么样呢？
3. 弱一致性，read() 可能读到旧的数据——不是最新写入的数据。
4. 强一致性，read() 始终能读到最新的写入的数据。
5. 权衡：强一致性对写操作友好但性能不佳。
6. 多种正确的条件，通常称为一致性模型（consistency model)。

一致性模型的历史：
1. 在架构，系统和数据库都有很多独立开发：
    1. 带有私有缓存的并发处理器，缓存是放在共享内存的。
    2. 并发的客户端访问一分布式文件系统。
    3. 分布式数据库的并发事务
2. 不同的模型有着不同权衡策略
    1. 可序列化
    2. 顺序一致性
    3. 可线性化
    4. 入口一致性
    5. 最终一致性

“理想”的一致性模型

1. 文件副本可以像没有副本的文件系统一样使用。如：多个客户端访问一台机器上某个磁盘上的文件。
2. 如果一个应用正在写入，之后的读取操作应该读到写入的内容。
3. 那么如果两个应用并发的写入同一个文件。在文件系统中没有定义——文件可能会有混合的内容。
4. 如果两个应用并发写入同一个目录，第一个先写，另一个后写。

不一致的原因：
1. 并发
2. 机器故障
3. 网络隔离

GFS 论文的示例：
1. primary 与 backup B 隔离
2. 客户端添加了 1
3. primary 向自己添加 1 并且向 A 中备份
4. 向客户端报告错误
5. 同时客户端 2 可能会向 B 中备份而且读到旧的数据

为什么理想模型在分布式文件系统很难实现？
1. 协议可能会很复杂——下节课回聊，正确的实现很难
2. 协议需要客户端和服务器沟通，这可能会降低性能

GFS 放弃实现理想模型转而设计了一个简单但性能更好的模型。

1. 但可能会让应用开发更痛苦
  1. 应用观察不可观察的行为
  2. 读到旧数据
  3. 重复添加记录
  4. 数据不是你的银行账户，也许没有太大问题

2. 现在的论文是在各种需求（一致性，性能，容错，简单设计）中挣扎。

GFS 的目标：

1. 创建一个共享文件系统
2. 可用于成百上千个廉价Linux机器。
3. 可存储巨量数据

GFS 存了什么数据？
猜测 2003: 搜索索引和数据库，web 上所有的 HTML 文件，web 上所有的图片。

文件的性质：
1. TB级别的数据
2. 大部分文件都很大
3. 1M 个 100M 的文件 = 100TB
4. 文件一般只有添加操作。

主要的挑战：
1. 大量机器宕机为经常发生的事情。
2. 高并发：大量读写并发操作。Map/Reduce 的读写数据都会进入 GFS。
3. 高效实用网络。

高级设计：
1. 目录，文件，命名，打开/读取/写入，没有遵循 POSIX
2. 以Linux 服务器磁盘块的100倍为单位
  1. 以 64MB 为块单位存储
  2. 每个块有三个副本分别存在三台机器上。
      1. Q: 为什么需要三个副本
      2. Q: 除了数据的可用性，三个副本还能带来什么好处？1. 热数据负载均衡，亲和力（affinity)
      3. Q: 为什么不选择只在 RAID 磁盘上存储一份？RAID 太贵，希望对这个机器容错，而不仅仅是存储设备。
      4. 为什么文件块这么大？
3. GFS master 服务器知道目录层级
  1. 目录里面有什么文件
  2. 在内存保存了每个 64MB 文件块所在的server信息，每个 64MB 文件块用 64 Byte 存储。
  3. master 用可恢复的数据库存储了 metadata，断电后可快速恢复。
  4. master 的一些副本可以晋升为新的master

基本操作

1. 客户端读：
  1. 向 master 发送文件名称和偏移，
  2. master 回复有那个文件块的服务器列表，客户端对这些数据会缓存一会
  3. 向最近的文件块服务器发送请求。
2. 客户端写：
  1. 询问 master 向哪台机器写
  2. 如果写入需求大于 64MB，master 也许有选择一组新的文件块服务器。
  3. 其中一个服务器是主要的。
  4. 它会选择更新的顺序以及2个备份。

两个不同的容错计划，对 master 和对 chunk 服务器

master 容错：
1. 单个 master
  1. 客户端只跟master通讯
  2. master 安排所有操作
2. 将状态持久化存储
  1. 命名空间，通常是目录
  2. 文件-chunk 的映射
3. 上面的两种数据的修改已log的形式存储
  1. log 有多个备份。
  2. 客户端的操作只有在其log被保存下来后才会执行
  3. log 在很多系统扮演了重要角色。
  4. 在我们的实验中 log 扮演了核心角色。
4. 限制 log 大小
  1. 对 master 的状态创建一个检查点
  2. 删除检查点之前所有的log信息，检查点本身是有备份的
5. 恢复
  1. 重上个检查点开始重新执行 log
  2. chunk 的位置信息从 chunk server 获得。
6. master 单点失败
  1. 恢复通常比较迅速，因为 master 的转台比较少，所有宕机时间比价少。
  2. master 备份服务器
    1. 他们log副本同步数据
    2. 只读操作，但可有返回旧数据
  3. 如果 master 不能被回复，master 会在其他机器中选举出来
  4. 必须小心以避免同时存在两个 master
7. 我们会看到有着更强保证的模型，当然也更复杂

Chunk 容错：

1. master 授权一个 chunk 作为同组副本中的 primary chunk 服务器，但有一定的租期。
2. Primary 决定操作执行顺序。
3. 客户端向副本组追加数据：
  1. 顺序遵循网络拓扑
  2. 可以快速复制
4. 客户端向 primary 发送写请求
  1. Primary 分配序列号
  2. Primary 本地写入
  3. Primary 将请求转给副本
  4. 当 Primary 收到所有副本都的 ASK 后，响应客户端
5. 如果一个副本没有响应，如果副本的数量低于某个值，客户端会尝试与 master 的副本沟通。
6. master 对副本进行负载均衡

Chunk 的一致性
1. 有些 chunk 可能会过期，丢失了一些改变
2. 用版本号判断数据是否过期
  1. 在发放租期之前增加版本号，将版本号发送给 primary 和副本 chunk 服务器。
  2. master 和 chunk 服务器将版本号持久化存储
3. 版本号也会发送给客户端
4. 版本号让master和客户端判断数据是否过期

并发写/追加
1. 客户端也许会并发地向一个文件的同一范围写入数据
2. 结果是那些写操作的混合——没有保证
  1. 很少有应用这么做，所以问题不大
  2. Unix 的并发写操作结果可能很奇怪。
3. 很多客户端可能想要并发追加，比如 log 文件
  1. GFS 支持原子的，至少一次 追加。
  2. primary chunk 服务器选择追在数据的偏移量
  3. 向所有副本发送这个副本
  4. 如果有副本失联，primary 会向client报告
  5. 客户端会重试，如果重试成功：有些副本会追加两次
  6. 文件可能会有洞，当 GFS 补 chunk 边界，如果一个追加可能会跨越 chunk 边界。

一致性模型
1. 目录操作是强一致性的
  1. master 修改 metadata 的操作是原子性的。
  2. 目录的操作是理想的，但是如果一个 master 离线了，master 的副本只能进行读操作，可能会返回旧数据。
2. chunk 的操作是弱一致性的。
  1. 错误的修改会让 chunk 不一致，primary chunk 服务器更新 chunk，但是如果失败了副本就过期了。
  2. 一个客户端可能读到过期的 chunk，如果客户端更新租期，它会得到新版本的数据。
3. 作者声称弱一致性对应用来说不是什么大问题。
  1. 大部分文件的更新是追加操作。应用可以用 UID 判断重复。应用可能读到的数据会少一些，但是并不是过期的数据。
  2. 应用可以更使用临时文件和原子性重命名。

性能
1. 巨大的读写综合生产力，125Mb/s，接近网络传速的饱和
2. 写入不同的文件速度比可能的最大值低，作者认为是他们的网络堆栈增加了 chunk 在副本中扩散的延迟。
3. 并发追加到同一文件，主要受限于服务器存储的最后一个chunk

总结
1. 使用 GFS 的重要 FT 技术。
  1. log 和 检查点
  2. chunks 副本，主从本分，有一致性。
2. GFS 适合什么工作？
  1. 大规模顺序读写，追加，巨大吞吐量，数据容错
3. GFS 什么地方不够好？
  1. master 容错
  2. 小文件是瓶颈
  3. 多客户端对同一文件并发修改（除了追加操作）
