6.824 2016 第一课：介绍

6.824: 分布式系统工程

什么是分布式系统？

1. 多台协作计算机
2. DNS，P2P文件分享，大规模数据库，MapReduce，&c 很多关键的基础设施都是分布式的。

为什么要分布式

1. 连接多个独立的物理实体
2. 通过隔离保障安全性
3. 通过复制实现容错
4. 通过并行的 CPU/内存/磁盘/网络 提升吞吐量

But：

1. 复杂性：很多并发运行的实体
2. 必须应对局部运行故障
3. 通过分布式提升性能潜力相当困难

为什么上这门课？

1. 有趣 —— 问题困难，没有直接了当的解决方案
2. 已应用于实际系统 —— 大规模的网站的现实需求驱动了分布式的发展
3. 热点研究领域 —— 长足进展 + 很多未解决的复杂问题
4. 时间行前 —— 你将在实验课课上构建符合规范的系统

### 课程结构

阅读：以研究论文作为案例学习

实验目标：
  
1. 加深对关键技术的理解
2. 积累分布式编程的经验

实验一：MapReduce

实验二：用复制来容错

实验三：带有容错机制的键值存储

实验四：共享的键值存储

### 主要研zhu主要究课题

三个只要的抽象概念：存储，通讯，计算。

1. 实现：RPC，线程，并发控制

2. 性能：
    1. 理想情况：可扩展的吞吐性能。N个服务器能通过并行CPU，磁盘和网络带来N倍的吞吐量。最终达到
  只要增加机器就能提升负载能力。
    2. 扩展越来越困难：负载均衡，失联的机器，无法并行的部分，隐藏共享资源，如网络。

3. 容错
    1. 1000+服务器，复杂的网络 —> 始终有机器发生故障
    2. 我们想在应用层面隐藏这些失败执行体
    3. 我们通常想要：
        1. 可用性：就算某些机器故障，我还是能用我的文件
        2. 持久化：故障机器修复后，我的数据能恢复
    4. 思路：复制机器，如果一台机器故障，客户端可以用其他机器继续工作

4. 一致性
    1. 通用的基础设施需要良好定义的行为，例如：Get(k) 得到最近 Put(k, v) 操作的v。
    2. 实现好的行为是相当困难的：客户端并发提交操作，服务器在尴尬的时候当机，网络故障让正常的机器看起来当机了，
  存在“裂脑”风险。
    3. 一致性和性能是冲突的
        1. 一致性需要通讯，如得到最近的put()
        2. 让人满意的系统通常都很慢
        3. 速度快的系统通常让应用应对复杂的系统调用
    4. 人们在这个领域尝试了很多设计模型

### 案例学习: MapReduce

#### 概述
  1. context: TB级别的数据，小时级别的预算量。例如：实验性的分析网络爬取页面的程序通常不是
  分布式爱好者开发的，因为这个痛苦，比如复制过程中出现错误。
  2. 整体目标：普通的程序员能在轻易地解决大量数据处理问题，并且有着合理的执行效率。
  3. 程序员定义Map和Reduce函数：顺序程序，通常很简单。
  4. MP 在 1000+ 台服务器用这些方法处理超大量的输入，隐藏了分布式所有的细节。

#### 抽象模型
  输入被分割成规模很小的碎片
  Input Map -> a,1 b,1 c,1
  Input Map ->     b,1
  Input Map -> a,1     c,1
                |   |   |
                    |   -> Reduce -> c,2
                    -----> Reduce -> b,2
  
  MR 调用 Map() 处理每个碎片，产生数据集合 (k2,v2) "中间"数据

  MR 搜集中间数据中所有k2对应的v2，然后将它们传给Reduce方法，最后从Reduce哪里得到 (k2, v3)

#### 示例：词频统计

输入是成千上万个文本文件

```
Map(k, v)
    split v into words
    for each word w
      emit(w, "1")
  Reduce(k, v)
    emit(len(v))
```

这个模型很容易编码；隐藏了很多令人痛苦的细节：

1. 并发，顺序执行得到结果
2. 在服务器上开启 s/w
3. 数据移动
4. 执行错误

#### 这个模型具有良好的扩展性
  1. Map() 不会等待其他 Map() 方法也不会分享数据，能并行执行。Reduce()也一样。在一定程度上。。。
  2. 所以多买点电脑就能提高吞吐量。而不是针对每个应用提高并行效率，毕竟机器比程序员便宜多了。

#### 什么因素会限制性能？CPU? 内存？磁盘？网络？
  1. 主要的限制因素是：网络横向带宽
  2. 很难构建一个比1000台机器还要快的网络，所以这个模型关心怎么最小化数据在网络之间的移动。 

#### 什么是容错？

  1. 也就是说，一个服务器在执行一个MR任务是当机了怎么办？
  2. 简化编程中的很大一部分是如何隐藏执行错误。
  3. 为什么不让机器从开始的地方重新执行这个任务？
  4. MR 仅仅重新执行失败的 Map() 和 Reduce()
    1. Map() 和 Reduce() 是纯方法 —— 他们没有修改输入，他们没有保存状态，没有共享内存，没有 map/map，reduce/reduce 交互。
    2. 所以重新执行极有可能得到相同的结果
  5. 相比其他并行编程的模式，对单纯函数的要求是 MR 的主要限制，但这也是 MR 简洁性的关键所在。

#### 更多细节（论文 图-1）
  
  1. master: 给 workers 分配任务；记住中间输出文件在哪里
  2. M 输入碎片，输入存在 GFS，每个碎片有3个副本，每个机器运行 GFS 和 MR worker，输入碎片的数量多于 worker，输入碎片的数量多于
  3. master 在每个机器上开启一个Map任务，当有机器完成时，接着分发新的任务。
  4. worker 把输出根据 key 散列到本地磁盘的 R 分区，所有的 Map 结束后，Reduce才会开始。
  5. master 告诉 Reducer 去拉取 Map 产生的中间数据。
  6. Reduce 将最终结构输入到 GFS。

#### 提高网络性能的具体设计是怎样的？

  1. Map 从本地磁盘读取输入数据，而不是从网络。
  2. 中间数据只通过网络传递一次，存在本地磁盘，而不是 GFS。
  3. 中间数据被分割成多个文件，每个文件保存多个key，传输文件比单个key传输更加高效。

#### 怎么做好负载均衡
  1. 扩展性是关键，不然 N 台服务器并没有效果。
  2. 但处理一个碎片数据或分区数据是不一致的，不同的数据量和内容还有服务器硬件也多种多样。
  3. 解决方案：碎片的数量大于worker的数量
      1. master 将新的碎片数据分发给也完成工作的worker
      2. 所以没有会控制结束时间的大碎片
      3. 所以运行速度快的服务器会比慢的服务器做更多的工作，整个任务消耗的时间差不多。

#### MR 如何应对 worker 当机

  1. Map 服务器当机：
      1. master 重新运行，通过 GFS 的其他副本的输入数据分发任务，甚至是 worker 已经完成了，因为还是需要在磁盘存入中间数据。
      2. 如果一些 Reduce 已经读取了失败的 worker 产生的中间数据，这里就得靠函数式和确定性的 Map()
      3. master是如何知道worker当机的？（ping)
      4. 如果 Reduce 已经读取了所有的中间数据，master就不必重新执行 Map 了，但如果 Reduce 失败了，Map 还是要再执行一边的。
  2. Reduce worker 在产生输出之前当机：master 再启动一个worker去执行这个任务
  3. Reduce worker 在写入输出的时候当机：GFS 在文件写入完成之前会修改文件名（原子操作），所以，master 重新执行 Reduce 也是安全的。

#### 其他的失败/问题：

  1. 要 master 偶尔开了两个 Map() 执行了相同的数据怎么办？
      - master 会告诉 Reduce worker 两个结果中的一个。
  2. 要是两个 Reduce worker 处理同一个中间数据的分区怎么办？
      - 它们两个都会尝试写到同一个 GFS 文件中，第二个写入操作会是最终结果。
  3. 要是个 worker 特别慢怎么办？可能由于奇怪的硬件。
      - master 会启动对最后的未完成的任务启动第二个 worker 执行这个任务。
  4. 要是由于损坏的 h/w 或 s/w 导致 worker 产生了错误的输出怎么办？
      - 这就很糟糕了，MR 假设 CUP 和 软件都是“出现错误就停止”的。
  5. 要是 master 当机了怎么办？

#### 什么应用不适合用 MapReduce 模型？

  1. 小数据，因为中间代价太高，例如，非后端网站。
  2. 向大量数据更新小数据，例如，向大索引加入一些文件
  3. 不可预测的读取
  4. 多次重组，例如，页面排名。

#### 结论
  1. 独立 MapReduce 让集群计算更加流行。
  1. 不是最高效和灵活的模式。
  1. 扩展性好。
  1. 编程简单 -- 错误和数据移动被隐藏
  1. 模型很好地权衡了实际应用中的需求


     

 

